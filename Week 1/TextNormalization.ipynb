{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c17b1f0-34e4-442e-a9ec-e94d063fe2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0c62d9-26b5-4ca8-943c-f50b9be21e1f",
   "metadata": {},
   "source": [
    "# Tokenization #\n",
    "### Tokenization is the process of breaking up the original text into components (tokens) ###\n",
    "\n",
    "## Spacy code ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4903bedf-6dc0-4bdc-abac-72a3b07de7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the language library\n",
    "nlp=spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5158daa9-0619-47db-b176-4070b00bd90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a document\n",
    "#parse each word string into token\n",
    "doc1=nlp(u'Lewis University located in U.S., Since 1932, Lewis University has been grounded in intentionality, guided by truth, and inspired by innovation.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "120ef54f-4363-4239-938e-a7a7f62c4d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lewis\n",
      "University\n",
      "located\n",
      "in\n",
      "U.S.\n",
      ",\n",
      "Since\n",
      "1932\n",
      ",\n",
      "Lewis\n",
      "University\n",
      "has\n",
      "been\n",
      "grounded\n",
      "in\n",
      "intentionality\n",
      ",\n",
      "guided\n",
      "by\n",
      "truth\n",
      ",\n",
      "and\n",
      "inspired\n",
      "by\n",
      "innovation\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "#Token has various attributes that we can grab from each token\n",
    "#For example, I can grab token text, which is the raw text that grabbed\n",
    "#It is smart enough to treat U.S. as a single token\n",
    "for token in doc1:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a04cc9d-57e8-4c22-972d-ca748fdf8c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lewis PROPN 96\n",
      "University PROPN 96\n",
      "located VERB 100\n",
      "in ADP 85\n",
      "U.S. PROPN 96\n",
      ", PUNCT 97\n",
      "Since SCONJ 98\n",
      "1932 NUM 93\n",
      ", PUNCT 97\n",
      "Lewis PROPN 96\n",
      "University PROPN 96\n",
      "has AUX 87\n",
      "been AUX 87\n",
      "grounded VERB 100\n",
      "in ADP 85\n",
      "intentionality NOUN 92\n",
      ", PUNCT 97\n",
      "guided VERB 100\n",
      "by ADP 85\n",
      "truth NOUN 92\n",
      ", PUNCT 97\n",
      "and CCONJ 89\n",
      "inspired VERB 100\n",
      "by ADP 85\n",
      "innovation NOUN 92\n",
      ". PUNCT 97\n"
     ]
    }
   ],
   "source": [
    "for token in doc1:\n",
    "    print(token.text, token.pos_, token.pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54fcb1d7-6292-4660-abe8-9c4b7702a02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lewis ===>  Lewis\n",
      "University ===>  University\n",
      "located ===>  locate\n",
      "in ===>  in\n",
      "U.S. ===>  U.S.\n",
      ", ===>  ,\n",
      "Since ===>  since\n",
      "1932 ===>  1932\n",
      ", ===>  ,\n",
      "Lewis ===>  Lewis\n",
      "University ===>  University\n",
      "has ===>  have\n",
      "been ===>  be\n",
      "grounded ===>  ground\n",
      "in ===>  in\n",
      "intentionality ===>  intentionality\n",
      ", ===>  ,\n",
      "guided ===>  guide\n",
      "by ===>  by\n",
      "truth ===>  truth\n",
      ", ===>  ,\n",
      "and ===>  and\n",
      "inspired ===>  inspire\n",
      "by ===>  by\n",
      "innovation ===>  innovation\n",
      ". ===>  .\n"
     ]
    }
   ],
   "source": [
    "# If we used lemma_, it will give you the limitization of the base from of the word\n",
    "for token in doc1:\n",
    "    print(f'{token.text} ===>  {token.lemma_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae8ed749-5f5d-416b-84da-036b392fb196",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize sentence\n",
    "doc2=nlp(u\"Welcome! this is the firstsentence. this is the second sentence. This is the last sentence \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "367c8afd-df79-4928-9613-790cc6bd4170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome!\n",
      "this is the firstsentence.\n",
      "this is the second sentence.\n",
      "This is the last sentence\n"
     ]
    }
   ],
   "source": [
    "for sen in doc2.sents:\n",
    "    print(sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7137740a-9acb-4aea-89c9-c7a86fd39ac7",
   "metadata": {},
   "source": [
    "## NLTK code ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2277afc7-5703-423d-bd58-e068829d89fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lewis', 'University', 'located', 'in', 'U.S.', ',', 'Since', '1932', ',', 'Lewis', 'University', 'has', 'been', 'grounded', 'in', 'intentionality', ',', 'guided', 'by', 'truth', ',', 'and', 'inspired', 'by', 'innovation', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text=\"Lewis University located in U.S., Since 1932, Lewis University has been grounded in intentionality, guided by truth, and inspired by innovation.\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "df565923-0399-4ec4-90da-521804831288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '!', 'NLP', 'is', 'amazing', '.', 'Is', \"n't\", 'it', '?']\n"
     ]
    }
   ],
   "source": [
    "text2 = \"Hello, world! NLP is amazing. Isn't it?\"\n",
    "tokens = word_tokenize(text2)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6b72054f-f895-4935-97b5-27320db2c55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello, world!', 'NLP is amazing.', \"Isn't it?\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sentences = sent_tokenize(text2)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c3571f-98b1-42fb-9e5f-3f7d978cfc3f",
   "metadata": {},
   "source": [
    "### Custom tokenization using regular expressions: ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a1056c2e-9680-4b7f-b2e8-e63ff420c8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')  # Words only (no punctuation)\n",
    "print(tokenizer.tokenize(\"Hello, world!\"))  # ['Hello', 'world']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "db8bf8ff-141a-4e17-b2d0-91fae40b2409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'costs', '$', '12.50', 'and', 'that', \"'s\", '50', '%', 'off', '!']\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "text3 = \"This costs $12.50 and that's 50% off!\"\n",
    "tokens3 = word_tokenize(text3)\n",
    "print(tokens3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d198c510-024c-4cab-8d7a-3117a5fa817f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '!']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\$?\\d+\\.?\\d+|\\w+|\\S') \n",
    "print(tokenizer.tokenize(\"Hello, world!\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ffc537-ae9e-4da7-8bfc-e163463afd5f",
   "metadata": {},
   "source": [
    "### Common Use Cases ###\n",
    "\n",
    "1- Text preprocessing for NLP pipelines\n",
    "\n",
    "2- Feature extraction for machine learning\n",
    "\n",
    "3- Word frequency analysis\n",
    "\n",
    "4- Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd646e97-03c2-43e7-a256-642d2cd2901e",
   "metadata": {},
   "source": [
    "# Stemming #\n",
    "### Porter Stemmer ###\n",
    "One of the most common - and effective - stemming tools is Porter's Algorithm developed by Martin Porter in 1980. The algorithm employs five phases of word reduction, each with its own set of mapping rules. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6630ebf2-f82c-49fe-8fb6-3e965672d4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the toolkit and the full Porter Stemmer library\n",
    "import nltk\n",
    "\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "af72886c-bde6-48b2-b21f-3d8c4db70a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pstemmer = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0af64bcf-f517-4455-b049-6f6f1fae9259",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['jump','run','running','runner', 'jumping', 'jumps', 'jumped', 'fairly','easily', 'happily', 'happiness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b7a218a6-b9d3-4681-8b29-b753384113e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jump', 'run', 'run', 'runner', 'jump', 'jump', 'jump', 'fairli', 'easili', 'happili', 'happi']\n"
     ]
    }
   ],
   "source": [
    "stemmed_words = [pstemmer.stem(word) for word in words]\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3ede14-70ff-4c20-89e7-c87e4829894e",
   "metadata": {},
   "source": [
    "Note how the stemmer recognizes \"runner\" as a noun, not a verb form or participle. Also, the adverbs \"fairly\", \"easily\", \"happily\" and \"happily\" are stemmed to the unusual root \"fairli\", \"easili\", \"happily\" and \"happi\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4a2c41-75a6-477f-9a97-81495c10de14",
   "metadata": {},
   "source": [
    "### Snowball Stemmer ###\n",
    "The algorithm implemented here is technically known as the \"English Stemmer\" or \"Porter2 Stemmer\", an enhanced version of the original Porter stemmer with optimizations in both logic and computational efficiency. While the nltk library refers to this implementation as SnowballStemmer, we will adhere to that naming convention for consistency in this context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2723cd46-d081-44e4-8c41-3d50fd0e918c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# The Snowball Stemmer requires that you pass a language parameter\n",
    "sstemmer = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "952723f3-e2b2-4a59-9f31-54308e526551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jump', 'run', 'run', 'runner', 'jump', 'jump', 'jump', 'fair', 'easili', 'happili', 'happi']\n"
     ]
    }
   ],
   "source": [
    "stemmed_words = [sstemmer.stem(word) for word in words]\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c06d9d7-f1aa-438f-80b3-f2562f95c983",
   "metadata": {},
   "source": [
    "<font color=blue>In this instance, the stemmer produced results identical to the Porter Stemmer, with one key improvement: it correctly derived the stem \"fair\" from \"fairly\", demonstrating more accurate handling of adverbial forms.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5a4e5e-8f34-4f7b-8167-5871af6e9a7c",
   "metadata": {},
   "source": [
    "# Lemmatization #\n",
    "\n",
    "Unlike stemming, which simply truncates word endings, lemmatization employs comprehensive linguistic analysis to determine a word's canonical form (lemma) based on its morphological structure and syntactic context. This process:\n",
    "\n",
    "1- References a language's complete lexical database\n",
    "\n",
    "2- Accounts for part-of-speech and semantic context\n",
    "\n",
    "3- Produces dictionary-valid words\n",
    "\n",
    "### Examples: ###\n",
    "\n",
    "'was' → 'be' (verb inflection)\n",
    "\n",
    "'mice' → 'mouse' (noun pluralization)\n",
    "\n",
    "'meeting' → either 'meet' (verb) or 'meeting' (noun), depending on usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685db65a-6139-4754-8c6e-8effabf66022",
   "metadata": {},
   "source": [
    "## Spacy ##\n",
    "Key Advantage: Uses part-of-speech (POS) context to determine the correct lemma.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b984dfea-d3b4-4ff0-888c-1dc295be5551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: The      | Lemma: the      | POS: DET\n",
      "Word: mice     | Lemma: mouse    | POS: NOUN\n",
      "Word: were     | Lemma: be       | POS: AUX\n",
      "Word: running  | Lemma: run      | POS: VERB\n",
      "Word: in       | Lemma: in       | POS: ADP\n",
      "Word: circles  | Lemma: circle   | POS: NOUN\n",
      "Word: ,        | Lemma: ,        | POS: PUNCT\n",
      "Word: meeting  | Lemma: meet     | POS: VERB\n",
      "Word: their    | Lemma: their    | POS: PRON\n",
      "Word: friends  | Lemma: friend   | POS: NOUN\n",
      "Word: .        | Lemma: .        | POS: PUNCT\n"
     ]
    }
   ],
   "source": [
    "text = \"The mice were running in circles, meeting their friends.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    print(f\"Word: {token.text:<8} | Lemma: {token.lemma_:<8} | POS: {token.pos_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f781ba1-6e28-4973-a11b-28b06bf21b41",
   "metadata": {},
   "source": [
    "## Using NLTK ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "27f726c6-061e-4919-82f9-8f3cc64cb4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "#from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7b7fa739-339c-49cd-9fa9-49f7a5768e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization Results: ['mouse', 'running', 'better', 'meeting']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = [\"mice\", \"running\", \"better\", \"meeting\"]\n",
    "lemmas = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "print(\"Lemmatization Results:\", lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5480510-d897-4f4a-a296-7a2d1987302a",
   "metadata": {},
   "source": [
    "### Lemmatization sentence ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7eccbdb6-3854-441c-95a1-28b958c807ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'mouse', 'were', 'running', 'in', 'circle', ',', 'meeting', 'their', 'friend', '.']\n"
     ]
    }
   ],
   "source": [
    "#text = \"The mice were running in circles, meeting their friends.\"\n",
    "def lemmatize_sentence(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
    "print(lemmatize_sentence(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725c7f9f-228b-4751-bfce-ded1f30a5c08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
